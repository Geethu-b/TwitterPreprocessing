{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes url from text\n",
    "import re\n",
    "import string\n",
    "def remove_url(tweet):\n",
    "    text = tweet\n",
    "    URL_REGEX = \"\"\"((?:(?:https|ftp|http)?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|org|uk)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|uk|ac)\\b/?(?!@)|((www\\.[^\\s]+)|(https?://[^\\s]+))))\"\"\"\n",
    "    newtext = re.sub(URL_REGEX, '', text, flags=re.MULTILINE)\n",
    "    lines = newtext.replace(\"\\n\",\"\")\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem ipsum  , nusquam tincidunt ex per,  ius modus integre no, quando utroque placerat qui no. Mea conclusionemque vituperatoribus et, omnes malorum est id, pri omnes atomorum expetenda ex. Elit  pertinacia no eos, nonumy comprehensam id mei. Ei eum maiestatis quaerendum  Pri posse constituam in, sit  omnium assentior definitionem ei. Cu duo equidem meliore qualisque.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Lorem ipsum  https://www.lore-m.com/ipsum.php?q=suas, \n",
    "nusquam tincidunt ex per, ftp://link.com ius modus integre no, quando utroque placerat qui no. \n",
    "Mea conclusionemque vituperatoribus et, omnes malorum est id, pri omnes atomorum expetenda ex. \n",
    "Elit ftp://link.work.in pertinacia no eos, nonumy comprehensam id mei. Ei eum maiestatis quaerendum www.lorem.orgüòÄ. \n",
    "Pri posse constituam in, sit www.news.bbc.co.de omnium assentior definitionem ei. Cu duo equidem meliore \n",
    "qualisque.\n",
    "\"\"\" \n",
    "remove_url(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love stack overflow because hungry people are very helpful'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing hashtag and splitting the words\n",
    "\n",
    "import re\n",
    "from wordsegment import load, segment\n",
    "load()\n",
    "\n",
    "# Hash fix - will be called from within hastag_sep_sent function\n",
    "def hash_fix(hashtag):\n",
    "    new_hash = re.sub(r'#', '', hashtag)\n",
    "    tokens = segment(str(new_hash))\n",
    "    hashed = ' '.join(map(str, tokens)) \n",
    "    return hashed\n",
    "\n",
    "def hastag_sep_sent(sent):\n",
    "    tokens = sent.split()\n",
    "    sentence = []\n",
    "    for word in tokens:\n",
    "        if word.startswith('#'):\n",
    "            hashedword = hash_fix(word)\n",
    "            sentence.append(hashedword)\n",
    "        else:\n",
    "            sentence.append(word)\n",
    "    return(\" \".join(sentence))\n",
    "        \n",
    "s=\"I love #stackoverflow because #hungrypeople are very #helpful!\"\n",
    "hastag_sep_sent(s)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "what a rush new life dreamz ! hurrah can not get enough . wtf\n"
     ]
    }
   ],
   "source": [
    "#another method for removing hashtag and splitting the words. also reduces extra puctuations and manages contacted words.\n",
    "\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "text_preprocessor = TextPreProcessor(annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "# corpus from which the word statistics are going to be used for word segmentation \n",
    "segmenter=\"twitter\", \n",
    "    \n",
    "# corpus from which the word statistics are going to be used for spell correction\n",
    "corrector=\"twitter\", \n",
    "unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "# select a tokenizer. You can use SocialTokenizer, or pass your own tokenizer, should take as input a string and return a list of tokens\n",
    "tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "# list of dictionaries, for replacing tokens extracted from the text,with other expressions. You can pass more than one dictionaries.\n",
    "dicts=[emoticons]\n",
    ")\n",
    "\n",
    "sentences = [\"what a rush #newlife #dreamz !!! HURRAH Can't get enough.\"]\n",
    "remove_words = ['<hashtag>','</hashtag>','<repeated>', '<elongated>', '<allcaps>' ,'</allcaps>']   \n",
    "\n",
    "for s in sentences:\n",
    "    tokens = text_preprocessor.pre_process_doc(s)\n",
    "    filtered_words = list(filter(lambda w: w not in remove_words, tokens))\n",
    "    print(\" \".join(filtered_words))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
