{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-ccd23a0f2468>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ccd23a0f2468>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def url(sentence):\n",
    "    String str = \"Today,wheather is cold. But I want to out.http://weathers.com..... And I will take a cup of tea\";\n",
    "    String regularExpression = \"(((http|ftp|https):\\\\/\\\\/)?[\\\\w\\\\-_]+(\\\\.[\\\\w\\\\-_]+)+([\\\\w\\\\-\\\\.,@?^=%&amp;:/~\\\\+#]*[\\\\w\\\\-\\\\@?^=%&amp;/~\\\\+#])?)\";\n",
    "    str = str.replaceAll(regularExpression,\"\");\n",
    "    System.out.println(str);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'replaceAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dbf2fa729cf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# str1 = str1.replaceAll(regularExpression,\"\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# System.out.println(str1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplaceAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularExpression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'replaceAll'"
     ]
    }
   ],
   "source": [
    "str1 = \"Today,wheather is cold. But I want to out. http://weathers.com..... And I will take a cup of tea\"\n",
    "regularExpression = \"(((http|ftp|https):\\\\/\\\\/)?[\\\\w\\\\-_]+(\\\\.[\\\\w\\\\-_]+)+([\\\\w\\\\-\\\\.,@?^=%&amp;:/~\\\\+#]*[\\\\w\\\\-\\\\@?^=%&amp;/~\\\\+#])?)\"\n",
    "# str1 = str1.replaceAll(regularExpression,\"\")\n",
    "# System.out.println(str1)\n",
    "str1.replaceAll(regularExpression, \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string:  <p>Contents :</p><a href=\"https://w3resource.com\">Python Examples</a><a href=\"http://github.com\">Even More Examples</a>\n",
      "Urls:  ['https://w3resource.com', 'http://github.com']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = '<p>Contents :</p><a href=\"https://w3resource.com\">Python Examples</a><a href=\"http://github.com\">Even More Examples</a>'\n",
    "urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "\n",
    "print(\"Original string: \",text)\n",
    "print(\"Urls: \",urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.lore-m.com/ipsum.php?q=suas', 'ftp://link.com', 'ftp://link.work.in', 'https://www.lorem.org', 'http://news.bbc.co.de']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "text = \"\"\"\n",
    "Lorem ipsum dolor sit amet https://www.lore-m.com/ipsum.php?q=suas, \n",
    "nusquam tincidunt ex per, ftp://link.com ius modus integre no, quando utroque placerat qui no. \n",
    "Mea conclusionemque vituperatoribus et, omnes malorum est id, pri omnes atomorum expetenda ex. \n",
    "Elit ftp://link.work.in pertinacia no eos, nonumy comprehensam id mei. Ei eum maiestatis quaerendum https://www.lorem.orgüòÄ. \n",
    "Pri posse constituam in, sit http://news.bbc.co.de omnium assentior definitionem ei. Cu duo equidem meliore \n",
    "qualisque.\n",
    "\"\"\"\n",
    "\n",
    "URL_REGEX = r\"\"\"((?:(?:https|ftp|http)?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|org|uk)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|uk|ac)\\b/?(?!@)))\"\"\"\n",
    "\n",
    "urls = re.findall(URL_REGEX, text)\n",
    "newtext = re.sub(URL_REGEX, '', text, flags=re.MULTILINE)\n",
    "# line = line.strip('\\n')\n",
    "# line = line.replace('\\n','')\n",
    "# print(urls)\n",
    "# print(x for x in urls if x in string.printable)\n",
    "url = [''.join(x for x in url if x in string.printable) for url in urls]\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlextract import URLExtract\n",
    "extractor = URLExtract()\n",
    "urls = extractor.find_urls(\"Let's have URL youfellasleepwhilewritingyourtitle.com as an example.\")\n",
    "print(urls) # prints: ['youfellasleepwhilewritingyourtitle.cz']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Gets the tweet time.\n",
    "def get_time(tweet):\n",
    "    return datetime.strptime(tweet['created_at'], \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "\n",
    "#Gets all hashtags.\n",
    "def get_hashtags(tweet):\n",
    "    return [tag['text'] for tag in tweet['entities']['hashtags']]\n",
    "\n",
    "#Gets the screen names of any user mentions.\n",
    "def get_user_mentions(tweet):\n",
    "    return [m['screen_name'] for m in tweet['entities']['user_mentions']]\n",
    "\n",
    "#Gets the text, sans links, hashtags, mentions, media, and symbols.\n",
    "def get_text_cleaned(tweet):\n",
    "    text = tweet['text']\n",
    "    \n",
    "    slices = []\n",
    "    #Strip out the urls.\n",
    "    if 'urls' in tweet['entities']:\n",
    "        for url in tweet['entities']['urls']:\n",
    "            slices += [{'start': url['indices'][0], 'stop': url['indices'][1]}]\n",
    "    \n",
    "    #Strip out the hashtags.\n",
    "    if 'hashtags' in tweet['entities']:\n",
    "        for tag in tweet['entities']['hashtags']:\n",
    "            slices += [{'start': tag['indices'][0], 'stop': tag['indices'][1]}]\n",
    "    \n",
    "    #Strip out the user mentions.\n",
    "    if 'user_mentions' in tweet['entities']:\n",
    "        for men in tweet['entities']['user_mentions']:\n",
    "            slices += [{'start': men['indices'][0], 'stop': men['indices'][1]}]\n",
    "    \n",
    "    #Strip out the media.\n",
    "    if 'media' in tweet['entities']:\n",
    "        for med in tweet['entities']['media']:\n",
    "            slices += [{'start': med['indices'][0], 'stop': med['indices'][1]}]\n",
    "    \n",
    "    #Strip out the symbols.\n",
    "    if 'symbols' in tweet['entities']:\n",
    "        for sym in tweet['entities']['symbols']:\n",
    "            slices += [{'start': sym['indices'][0], 'stop': sym['indices'][1]}]\n",
    "    \n",
    "    # Sort the slices from highest start to lowest.\n",
    "    slices = sorted(slices, key=lambda x: -x['start'])\n",
    "    \n",
    "    #No offsets, since we're sorted from highest to lowest.\n",
    "    for s in slices:\n",
    "        text = text[:s['start']] + text[s['stop']:]\n",
    "        \n",
    "    return text\n",
    "\n",
    "#Sanitizes the text by removing front and end punctuation, \n",
    "#making words lower case, and removing any empty strings.\n",
    "def get_text_sanitized(tweet):    \n",
    "    return ' '.join([w.lower().strip().rstrip(string.punctuation)\\\n",
    "        .lstrip(string.punctuation).strip()\\\n",
    "        for w in get_text_cleaned(tweet).split()\\\n",
    "        if w.strip().rstrip(string.punctuation).strip()])\n",
    "\n",
    "#Gets the text, clean it, make it lower case, stem the words, and split\n",
    "#into a vector. Also, remove stop words.\n",
    "def get_text_normalized(tweet):\n",
    "    #Sanitize the text first.\n",
    "    text = get_text_sanitized(tweet).split()\n",
    "    \n",
    "    #Remove the stop words.\n",
    "    text = [t for t in text if t not in stopwords.words('english')]\n",
    "    \n",
    "    #Create the stemmer.\n",
    "    stemmer = LancasterStemmer()\n",
    "    \n",
    "    #Stem the words.\n",
    "    return [stemmer.stem(t) for t in text]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-01de3ca5f148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mqualisque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mget_hashtags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-01de3ca5f148>\u001b[0m in \u001b[0;36mget_hashtags\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_hashtags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hashtags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m tweet = \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0mLorem\u001b[0m \u001b[0mipsum\u001b[0m \u001b[0;31m#dolor sit amet https://www.lore-m.com/ipsum.php?q=suas,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnusquam\u001b[0m \u001b[0mtincidunt\u001b[0m \u001b[0mex\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m \u001b[0mius\u001b[0m \u001b[0mmodus\u001b[0m \u001b[0mintegre\u001b[0m \u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquando\u001b[0m \u001b[0mutroque\u001b[0m \u001b[0mplacerat\u001b[0m \u001b[0mqui\u001b[0m \u001b[0mno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "def get_hashtags(tweet):\n",
    "    return [tag['text'] for tag in tweet['entities']['hashtags']]\n",
    "tweet = \"\"\"\n",
    "Lorem ipsum #dolor sit amet https://www.lore-m.com/ipsum.php?q=suas, \n",
    "nusquam tincidunt ex per, ftp://link.com ius modus integre no, quando utroque placerat qui no. \n",
    "Mea conclusionemque vituperatoribus et, omnes malorum est id, pri omnes atomorum expetenda ex. \n",
    "Elit ftp://link.work.in pertinacia no eos, nonumy comprehensam #id mei. Ei eum maiestatis quaerendum https://www.lorem.orgüòÄ. \n",
    "Pri posse constituam in, sit http://news.bbc.co.uk omnium assentior definitionem ei. Cu duo equidem meliore \n",
    "qualisque.\n",
    "\"\"\"\n",
    "get_hashtags(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "what a rush new life dreamz ! hurrah can not get enough .\n"
     ]
    }
   ],
   "source": [
    "# import ekphrasis\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "text_preprocessor = TextPreProcessor(annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "# corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "corrector=\"twitter\", \n",
    "    \n",
    "unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "dicts=[emoticons]\n",
    ")\n",
    "sentences = [\"what a rush #newlife #dreamz !!! HURRAH Can't get enough.\"]\n",
    "remove_words = ['<hashtag>','</hashtag>','<repeated>', '<elongated>', '<allcaps>' ,'</allcaps>']   \n",
    "for s in sentences:\n",
    "    tokens = text_preprocessor.pre_process_doc(s)\n",
    "    filtered_words = list(filter(lambda w: w not in remove_words, tokens))\n",
    "    print(\" \".join(filtered_words))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "sentences = [\n",
    "    \"CANT WAIT for the new season of #TwinPeaks Ôºº(^o^)Ôºè!!! #davidlynch #tvseries :)))\",\n",
    "    \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
    "    \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    print(\" \".join(text_processor.pre_process_doc(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love stack overflow because hungry people are very helpful\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from wordsegment import load, segment\n",
    "load()\n",
    "\n",
    "# Hash fix - will be called from within tweet cleaning function\n",
    "def hash_fix(hashtag):\n",
    "#     h1 = re.sub(r'[0-9]+', '', h)\n",
    "    new_hash = re.sub(r'#', '', hashtag)\n",
    "#     print(new_hash)\n",
    "    tokens = segment(str(new_hash))\n",
    "#     print(tokens)\n",
    "    hashed = ' '.join(map(str, tokens)) \n",
    "    return hashed\n",
    "\n",
    "s=\"I love #stackoverflow because #hungrypeople are very #helpful!\"\n",
    "tokens = s.split()\n",
    "sentense = []\n",
    "for word in tokens:\n",
    "    if word.startswith('#'):\n",
    "        hashedword = hash_fix(word)\n",
    "        sentense.append(hashedword)\n",
    "    else:\n",
    "        sentense.append(word)\n",
    "print(\" \".join(sentense))\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
